{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Contents\n",
    "<font size = 3>\n",
    "\n",
    "<a href=\"#intro\">Introduction</a>\n",
    "\n",
    "0. <a href=\"#item0\">Imports</a>\n",
    "\n",
    "1. <a href=\"#item1\">Data Acquisition and Preparation</a>\n",
    "\n",
    "2. <a href=\"#item2\">Data Cleaning</a>\n",
    "\n",
    "3. <a href=\"#item3\">Using Google Places API</a>\n",
    "\n",
    "4. <a href=\"#item4\">Exploratory Data Analysis</a>\n",
    "\n",
    "5. <a href=\"#item5\">Four Square API</a>\n",
    "    \n",
    "6. <a href=\"#item6\">Clustering</a>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a> \n",
    "# Introduction to problem:\n",
    "\n",
    "## Neighbourhood Similarity near Colleges \n",
    "<p>\n",
    "Every year many graduates pursue masters, if we consider us (people doing the course), many of us would want to go for \"Computer Science & Information Technology\" field. The top choices for us would be USA, UK, Canada (intentionally ignoring Germany because German is compulsory in it). What many people want is a proper neighborhood (ignoring money constraints), once you do find a good college, you might want to know what lies near the college, we'll use FourSquare API to explore the neighbourhood and find colleges having the same neighbourhood. So if you don't get selected in the college you would know what colleges have similar neighbourhood, makes it easy to choose what colleges to choose to send application based on your neighbourhood preference. For example I want a safe neighbourhood, with many cafes and gyms (suppose) so I would explore neighbourhood BASED on my preferences, and so can everyone else. This is useful for any student who wants to goto a college with a similar neighbourhood in or to other country.\n",
    "</p>\n",
    "\n",
    "## Data Used\n",
    "\n",
    "We are using www.mastersportal.com to mine our data.\n",
    "We are searching for Canadian, Australian, United States, United Kingdoms Colleges/ Universities that provide full time Master's Degree Programme in the field of Computer Science and IT.\n",
    "We have the following details available:\n",
    "Degree,\n",
    "Density,\n",
    "Full Time Duration,\n",
    "ID,\n",
    "level,\n",
    "listing_type,\n",
    "logo,\n",
    "organization,\n",
    "organization_id,\n",
    "summary,\n",
    "tution fee,\n",
    "Address: area, city, country.\n",
    "\n",
    "Data we'll be requiring:\n",
    "organization_id: College id<br> \n",
    "organization: College name<br> \n",
    "tution fee: College Fees<br>\n",
    "Address: area, city, country of College<br>\n",
    "latitude, longitude: can be obtained from address using geopy<br>\n",
    "\n",
    "## Usage of Data (used for)\n",
    "organization_id: Primary Key (in case colleges have same name)<br> \n",
    "organization: Name to refer to college<br> \n",
    "tution fee: Find colleges with acceptable fee range<br>\n",
    "Address: Find Lat-Long of college<br>\n",
    "latitude, longitude: For clustering and exploring with FourSquare Places API<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<a id='item0'></a> \n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests # get requests\n",
    "import json # to parse the json file\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd # because arrays are oldschool \n",
    "from pandas.io.json import json_normalize # for handling nested json\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "from GoogleMapsApiKey import get_key\n",
    "import FoursquareApiCredentials as fs\n",
    "\n",
    "\n",
    "from progressbar import ProgressBar # tests your patience\n",
    "import time # for delay\n",
    "from IPython.display import clear_output # to clear notebook output cell via code\n",
    "\n",
    "print('Libraries Imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<a id='item1'></a> \n",
    "# 1. Data Acquisition and Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data by web scraping\n",
    "## We are using <a href= \"https://www.mastersportal.com/\"> www.mastersportal.com</a> to mine our data.\n",
    "<p>We are searching for Canadian, Australian, United States, United Kingdoms Colleges/ Universities that provide full time Master's Degree Programme in the field of Computer Science and IT.</p>\n",
    "\n",
    "## Initial look at the data.\n",
    "\n",
    "#### The search looked like this: <a href = \"https://github.com/xtreme0021/Capstone/blob/master/Images/search_tags_smallext.png\">https://github.com/xtreme0021/Capstone/blob/master/Images/search_tags_smallext.png</a>\n",
    "\n",
    "#### We inspect-element the search results: <a href = \"https://github.com/xtreme0021/Capstone/blob/master/Images/rmbMenu.png\">https://github.com/xtreme0021/Capstone/blob/master/Images/rmbMenu.png</a>\n",
    "\n",
    "#### The inspector shows the following: <a href = \"https://github.com/xtreme0021/Capstone/blob/master/Images/inspectElement.png\">https://github.com/xtreme0021/Capstone/blob/master/Images/inspectElement.png</a>\n",
    "\n",
    "<p>So the Information is contained in the span tag, class = 'Location'. But, if we 'View Page Source' of the HTML and search for 'Location' this is what it shows:\n",
    "<br><br>\n",
    "<textarea rows=\"7\" cols=\"50\">\n",
    "    <span class=\"Location\"> \n",
    "        <span class=\"Fact LocationFact\">\n",
    "               {{organisation}}\n",
    "        </span> \n",
    "        <span class=\"Fact LocationFact\">\n",
    "            {{{venue}}}\n",
    "        </span>\n",
    "    </span>\n",
    "</textarea>\n",
    "<br><br>\n",
    "The <font color=\"red\">{{organization}}, {{{venue}}}</font> tags imply that the data is being dynamically inserted into the HTML  via a JSON file, so lets get the JSON. \n",
    "</p>\n",
    "\n",
    "## Getting the JSON\n",
    "\n",
    "<p>\n",
    "1. Goto inspect element.<br>\n",
    "2. Choose Network Tab.<br>\n",
    "3. Goto 2nd Page of search to refresh network activity.<br>\n",
    "4. We are looking for a json that fills up search requests. So the column Type would be json, and since we are requesting for data, the network fetches data so cause is fetch.<br>\n",
    "5. One of the Domains fullfills our needs, the 'search.prtl.co' domain. Double click it. Inspector-NetworkActivity:\n",
    "<a href = \"https://github.com/xtreme0021/Capstone/blob/master/Images/NetworkActivity.png\">https://github.com/xtreme0021/Capstone/blob/master/Images/NetworkActivity.png</a><br>\n",
    "6. Voila! You have the link of a json string that generates the result.<br>\n",
    "7. We can save the json from there but well good luck with 214 files :). (Don't worry i have a solution.)<br>\n",
    "</p>\n",
    "\n",
    "## Notice that we were on the second page and the url for out json was: \n",
    "\n",
    "<a href=\"https://search.prtl.co/2018-07-23/?start=10&q=ci-30%2C56%2C202%2C82%7Cdg-msc%7Cde-fulltime%7Cdi-24%7Cen-413%7Clv-master%7Ctc-EUR\"> https://search.prtl.co/2018-07-23/?start=10&q=ci-30%2C56%2C202%2C82%7Cdg-msc%7Cde-fulltime%7Cdi-24%7Cen-413%7Clv-master%7Ctc-EUR</a><br></code>\n",
    "\n",
    "<p>\n",
    "So the '?start=10' represents the college to start from in the returned search list. If we put 20 in place of 10 then it gives us the data of 3rd page, so now we can have data for all pages by changing 'start' value. The max start value should be 2130 i.e college no 2131 - 2137. (total search results were 2137 (on the day I'm searching on)if you saw in search image).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are 2137 colleges, say 2140 for simplicity sake and in each page there are 10 colleges.\n",
    "jsonExportFileName = 'college-list.json'\n",
    "with open('Capstone/JSON/' + jsonExportFileName, 'a+') as jsonFile:\n",
    "    for i in range(0, 2140, 10): # (start_value, approx total colleges, no_of_college_per_page)\n",
    "        # using formatted strings to generate url for search returned json text, we are doing ?start=i, to get first 10 results, then next ten,till 2137 results are obtained.\n",
    "        url = \"https://search.prtl.co/2018-07-23/?start={}&q=ci-30%2C56%2C202%2C82%7Cdg-msc%7Cde-fulltime%7Cdi-24%7Cen-413%7Clv-master%7Ctc-EUR\".format(str(i)) \n",
    "        webPage = requests.get(url) # connect and get the WebPage\n",
    "        \n",
    "        json.dump(webPage.json(), jsonFile) # dump formatted json data from webPage to jsonFile\n",
    "print(jsonExportFileName+\" has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the data is coming from 213 webpages, the json file strings ends 213 times i.e theres '][' (end and start of json) in between which should be ',' (comma to continue the json).\n",
    "##### So I manully replaced them using the editor. A simple find and replace would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Capstone/JSON/college-list.json') as jsonFile: # refers to the json we created while scraping the website\n",
    "    raw_data = json.load(jsonFile) # load data to a python variable\n",
    "print('Data Imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = json_normalize(raw_data) # normalizing the data using pandas library function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'venues' is nested, so we normalize it to a different dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = json_normalize(data = raw_data, record_path = 'venues')\n",
    "venues.drop('display_area', axis = 1, inplace = True) # this column serves no purpose whatsoever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check if we successfully extracted the venues data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(venues) # Merging df and venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<a id='item2'></a> \n",
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # lets revise the columns we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We seriously dont need these columns they are just clutter that we got from json we parsed\n",
    "columns_to_drop = ['degree', 'density.fulltime', 'density.parttime', \n",
    "    'enhanced', 'organisation_id', 'level', 'listing_type', 'logo', 'methods.blended',\n",
    "    'methods.face2face', 'methods.online','parttime_duration.unit', \n",
    "    'parttime_duration.value', 'summary', 'title', 'venues', 'fulltime_duration.value', 'fulltime_duration.unit']\n",
    "df.drop(columns_to_drop, axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['tuition_fee.currency'] != 'EUR'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tuition_fee.currency'].notnull()] #new df from current df where tuition_fee.currency is not null \n",
    "df = df[df['area'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://www.geteducated.com/career-center/detail/what-is-a-masters-degree,\n",
    "To earn a master’s degree you usually need to complete from 36 to 54 semester credits of study (or 60 to 90 quarter-credits). This equals 12 to 18 college courses. \n",
    "\n",
    "45 is average of 36 ad 54!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['tuition_fee.unit'] == 'credit', 'tuition_fee.value'] = (df['tuition_fee.value']*45)/2 \n",
    "# Multiplying tuition_fee.value by 45 when tuition_fee.unit is 'credit' \n",
    "# This gives us average per year fees, to get a uniform fee scale (all fees in per year format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['tuition_fee.currency', 'tuition_fee.unit'], axis=1, inplace=True) \n",
    "# since we have uniform values we dont need the currency and unit thus we will drrop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'tuition_fee.value': 'fees', 'organisation': 'college_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging the columns\n",
    "df = df[['id','college_name', 'fees', 'area', 'city', 'country']] # removed location from here on date 20191023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()\n",
    "df_test['clg_city'] = df['college_name'].map(str)+', '+df['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clg_names = df['college_name'].to_list()\n",
    "clg_city = df_test['clg_city'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<a id='item3'></a> \n",
    "# 3. Using Google Places API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_progress = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPlace(query, key):\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?'\n",
    "    req = requests.get(\n",
    "        url + \n",
    "        'input='  + query +\n",
    "        '&inputtype='+ 'textquery' +\n",
    "        '&fields=' + 'geometry/location'+\n",
    "        '&key=' + key\n",
    "    ) \n",
    "    time.sleep(0.02)\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jdump_latlongG(filename, query_list, key):\n",
    "    with open(filename, 'a+') as jsonFile:\n",
    "        for i in show_progress(range(0, 2060)):\n",
    "            req = findPlace(query_list[0][i], key)\n",
    "            if req.json()['status'] != 'OK':\n",
    "                req = findPlace(query_list[1][i], key)\n",
    "            json.dump(req.json(), jsonFile)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdump_latlongG('G-lat-long-new.json', [clg_city, clg_names], API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Importing latitudes and longitudes from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G-lat-long-new.json') as jsonFile: # refers to the json we created earlier\n",
    "    ll_data = json.load(jsonFile) # load data to a python var\n",
    "print('Lat-Long Imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = json_normalize(ll_data, record_path='candidates', meta =['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.drop('formatted_address', axis=1, inplace=True)\n",
    "ll.rename(columns={'geometry.location.lat': 'latitude', 'geometry.location.lng': 'longitude'})\n",
    "ll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Making new columns in df from ll dataframe\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['status']==\"OK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['status'], axis=1, inplace=True)\n",
    "df = df.rename(columns={'geometry.location.lat': 'latitude', 'geometry.location.lng': 'longitude'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('college-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## We have saved the data to college_dataset.csv \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"college-dataset.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['country'].str.contains(\"United States|United Kingdom|Canada\")] \n",
    "# keeping only the locations with location-country as US, UK, Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked lat long on google maps\n",
    "df['latitude'][df.college_name=='University of California, Berkeley'] = 37.8718992\n",
    "df['longitude'][df.college_name=='University of California, Berkeley'] = -122.2607286\n",
    "\n",
    "df['latitude'][df.college_name=='Johnson & Wales University'] = 41.8197902\n",
    "df['longitude'][df.college_name=='Johnson & Wales University']= -71.415209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They are placed in india and nowhere found on google maps\n",
    "df.drop(df[df['college_name']=='Engineering and Technology College'].index, inplace = True)\n",
    "df.drop(df[df['college_name']=='College of Nursing and Public Health'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['fees']>50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually searched college fees\n",
    "df['fees'][df.college_name=='University of Colorado at Boulder'] = 48570\n",
    "df['fees'][df.college_name=='University of Pennsylvania'] = 7134\n",
    "df['fees'][df.college_name=='University of Nebraska Omaha'] = 28564\n",
    "df['fees'][df.college_name=='Johns Hopkins University'] = 45350\n",
    "df['fees'][df.college_name=='University of Illinois at Urbana Champaign'] = 53437\n",
    "df['fees'][df.college_name=='Carnegie Mellon University'] = 38940\n",
    "df['fees'][df.college_name=='Northwestern University'] = 42000\n",
    "df['fees'][df.college_name=='Washington University in St. Louis'] = 63000\n",
    "df['fees'][df.college_name=='University of Massachusetts Amherst'] = 62000\n",
    "df['fees'][df.college_name=='Bentley University'] = 68640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final-college-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "<a id='item4'></a> \n",
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our data it's time to explore it. Lets see the number per country. The venues data frame would make it easy to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_college_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa=df[df['country'].str.contains(\"United States\")]\n",
    "df_uk=df[df['country'].str.contains(\"United Kingdom\")]\n",
    "df_canada=df[df['country'].str.contains(\"Canada\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Lets look at the geospatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_coordinates = [37.0902, -100]\n",
    "canada_coordinates = [54.6959279, -90]\n",
    "uk_coordinates = [54.2186138, -5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## USA Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_map = folium.Map(location = usa_coordinates, zoom_start = 4)\n",
    "mc = MarkerCluster().add_to(usa_map)    \n",
    "\n",
    "for row in df_usa.itertuples():\n",
    "    folium.Marker(\n",
    "        location=[row.latitude,row.longitude],\n",
    "        icon = None,\n",
    "        popup=row.college_name\n",
    "    ).add_to(mc)\n",
    "\n",
    "usa_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case map not showing: https://github.com/xtreme0021/Capstone/blob/master/Images/USA_marker_cluster.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Canada Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_map = folium.Map(location = canada_coordinates, zoom_start = 4)\n",
    "mc = MarkerCluster().add_to(canada_map)    \n",
    "\n",
    "for row in df_canada.itertuples():\n",
    "    folium.Marker(\n",
    "        location=[row.latitude,row.longitude],\n",
    "        icon = None,\n",
    "        popup=row.college_name\n",
    "    ).add_to(mc)\n",
    "\n",
    "canada_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case map not showing: https://github.com/xtreme0021/Capstone/blob/master/Images/Canada_marker_cluster.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## UK Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_map = folium.Map(location = uk_coordinates, zoom_start = 6)\n",
    "mc = MarkerCluster().add_to(uk_map)    \n",
    "\n",
    "for row in df_uk.itertuples():\n",
    "    folium.Marker(\n",
    "        location=[row.latitude,row.longitude],\n",
    "        icon = None,\n",
    "        popup=row.college_name\n",
    "    ).add_to(mc)\n",
    "\n",
    "uk_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case map not showing: https://github.com/xtreme0021/Capstone/blob/master/Images/UK_marker_cluster.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Lets Take a look at the salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(3,6)})\n",
    "boxplot = sns.boxplot(data=df['fees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at average fees by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average fees per country')\n",
    "print(\"USA : {0:.2f}\".format(df_usa['fees'].mean()))\n",
    "print(\"UK : {0:.2f}\".format(df_uk['fees'].mean()))\n",
    "print(\"Canada : {0:.2f}\".format(df_canada['fees'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(7,6)})\n",
    "boxplot = sns.boxplot(\n",
    "    data = df,\n",
    "    x = 'country',\n",
    "    y = 'fees'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: \n",
    "\n",
    "## The box plot of Canada lies within the 50% Quartile range of USA, it won't be surprising if we get more <font color = 037ffc>Canadian colleges</font> when looking for similar <font color = 037ffc>low fees</font> colleges when holding money as a criterion.\n",
    "\n",
    "## Similarly, <font color = 037ffc>UK</font> would be a preferred choice when choosing <font color = 037ffc>mid to high fees </font>colleges when compared to USA when holding money as a criterion.\n",
    "\n",
    "## It is obvious, but let me specifically point out, <font color = 'red'>USA has the highest fees</font> in all 3 nations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<a id='item5'></a> \n",
    "# 5. FourSquare Places API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = fs.get_client_id() # your Foursquare ID\n",
    "CLIENT_SECRET = fs.get_client_secret() # your Foursquare Secret\n",
    "VERSION = '20181102' # Foursquare API version\n",
    "RADIUS = 10000 # Radius to search in\n",
    "LIMIT = 20 # Limit to no. of search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=10000):\n",
    "    count = 0\n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(count, name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        try:\n",
    "            results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "            venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['shortName']) for v in results])\n",
    "        \n",
    "        except KeyError:\n",
    "            print('KeyError: Replacing with none.')\n",
    "            venues_list.append([name, lat, lng, None, None, None, None])\n",
    "        count += 1\n",
    "    \n",
    "    return(venues_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "college_venues = getNearbyVenues(names=df['college_name'],\n",
    "                                   latitudes=df['latitude'],\n",
    "                                   longitudes=df['longitude']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "none_list = []\n",
    "for i in range(len(college_venues)+1):\n",
    "    try:\n",
    "        if college_venues[i][-1] == None:\n",
    "            none_list.append(i)\n",
    "    except:\n",
    "        pass\n",
    "none_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_none = []\n",
    "for i in range(len(college_venues)):\n",
    "    if i not in none_list:\n",
    "        not_none.append(college_venues[i])\n",
    "    else:\n",
    "        pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues = pd.DataFrame([item for venue_list in not_none for item in venue_list])\n",
    "nearby_venues.columns = ['college_name', \n",
    "                  'c_latitude', \n",
    "                  'c_longitude', \n",
    "                  'venue', \n",
    "                  'v_latitude', \n",
    "                  'v_longitude', \n",
    "                  'v_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(none_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues.groupby('college_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(nearby_venues['v_category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<a id='item6'></a> \n",
    "# 6. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_onehot = pd.get_dummies(nearby_venues[['v_category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add college_name column back to dataframe\n",
    "nearby_onehot['college_name'] = nearby_venues['college_name'] \n",
    "\n",
    "# move college_name column to the first column\n",
    "fixed_columns = [nearby_onehot.columns[-1]] + list(nearby_onehot.columns[:-1])\n",
    "nearby_onehot = nearby_onehot[fixed_columns]\n",
    "\n",
    "nearby_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_grouped = nearby_onehot.groupby('college_name').mean().reset_index()\n",
    "nearby_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['college_name']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "nearby_venues_sorted = pd.DataFrame(columns=columns)\n",
    "nearby_venues_sorted['college_name'] = nearby_grouped['college_name']\n",
    "\n",
    "for ind in np.arange(nearby_grouped.shape[0]):\n",
    "    nearby_venues_sorted.iloc[ind, 1:] = return_most_common_venues(nearby_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "nearby_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "nearby_grouped_clustering = nearby_grouped.drop('college_name', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(nearby_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "nearby_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "nearby_merged = df\n",
    "\n",
    "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "nearby_merged = nearby_merged.join(nearby_venues_sorted.set_index('college_name'), on='college_name')\n",
    "\n",
    "nearby_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_merged[nearby_merged['Cluster Labels'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_merged.drop(619, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=usa_coordinates, zoom_start=3)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(nearby_merged['latitude'], nearby_merged['longitude'], nearby_merged['college_name'], nearby_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[int(cluster-1)],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[int(cluster-1)],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in case map not showing: https://github.com/xtreme0021/Capstone/blob/master/Images/Cluster.png\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_merged.loc[nearby_merged['Cluster Labels'] == 0, nearby_merged.columns[[1] + list(range(5, nearby_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_merged.loc[nearby_merged['Cluster Labels'] == 1, nearby_merged.columns[[1] + list(range(5, nearby_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_merged.loc[nearby_merged['Cluster Labels'] == 2, nearby_merged.columns[[1] + list(range(5, nearby_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_merged.loc[nearby_merged['Cluster Labels'] == 3, nearby_merged.columns[[1] + list(range(5, nearby_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_merged.loc[nearby_merged['Cluster Labels'] == 4, nearby_merged.columns[[1] + list(range(5, nearby_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Conclusion:\n",
    "The colleges have been genuinely clustered on the basis of their neighbourhood, and have an indefinite trend. From the clusters it is apparent that the neighbourhood of UK will majorly defer from that of USA or Canada. Hence, I’ve successfully clustered the college neighbourhood into the following categories:\n",
    "1.\tAmerican Eats\n",
    "2.\tExotic Eats\n",
    "3.\tTour/ Outgoing\n",
    "4.\tNight Life (Pub) and Fitness\n",
    "5.\tArt Prone/ Mature Audience\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
